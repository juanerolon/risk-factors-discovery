{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Approach to the Discovery of Disease Risk Factors on Individuals Assessed by the National Health and Nutrition Examination Surveys 2013-2014\n",
    "\n",
    "### Juan E. Rolon\n",
    "e-mail: rolon.math@gmail.com\n",
    "\n",
    "Original capstone project submitted to Udacity, Inc. as partial fulfillment to obtain the Machine Learning Nanodegree certification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of **chronic disease risk factors** is one of the most important and challenging problems in healthcare analytics [1]. Accurate risk factor prediction helps developing strategies aimed at avoiding unnecessary interventions for patients and reducing costs for insurance companies and healthcare providers. \n",
    "\n",
    "An important step towards risk factor prediction is the identification of individuals who would benefit from preventive treatment. Machine-learning algorithms are strongly suited for this task as they can improve identification accuracy by exploiting complex interactions between different risk factors and patient data features associated to disease [2]. \n",
    "\n",
    "In particular, supervised and unsupervised learning techniques can be applied to extract risk-associated features from readily available healthcare data, including demographic and wellness data, lifestyle behavior and health status markers.\n",
    "\n",
    "Among the chronic diseases that are the focus of intense research in the domain of healthcare\n",
    "analytics are **diabetes** and **cardiovascular disease (CVD).** In the United States, estimates indicate the existence of 30.3 million people with diabetes (9.4% of the US population) including 23.1 million people who are diagnosed and 7.2 million people (23.8%) undiagnosed. The numbers for prediabetes indicate that 84.1 million adults (33.9% of the adult U.S. population) have prediabetes, including 23.1 million adults aged 65 years or older (the age group with highest rate). \n",
    "\n",
    "Furthermore, about 610,000 people die of cardiovascular disease in the United States every year, that is 1 in every 4 deaths. In particular, heart disease is the leading cause of death for both men and women [3, 4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a **gradient boosting classifier** on publicly available data, to identify risk predictors for diabetes and cardiovascular disease for individuals residing in the United States.  Our results suggest that researchers and clinicians could make use of machine learning analyses to obtain valuable health assessment of patients at a much reduced cost and time. \n",
    "\n",
    "The data for our study were extracted from a population sample of individuals who participated in the **National Health and Nutrition Examination Surveys (NHANES) 2013-2014.** \n",
    "\n",
    "The present problem is framed as a **binary classification task**, in which the positive class are individuals presenting the disease, while the negative class are individuals not presenting the disease. The model's solution consists of the following components\n",
    "\n",
    "1.\tCompilation and pre-processing of clinical datasets containing personal health status data.\n",
    "2.\tAccurate classification of a surveyed person as having risk or no risk for a target disease.\n",
    "3. Identification of important features (risk factors) that serve as main predictors of disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Datasets and Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The NHANES datasets contain unique records on demographic, health status, eating behavior and blood biochemistry data of surveyed individuals. \n",
    "\n",
    "\n",
    "- The NHANES datasets are open source and unrestricted and are available to the public for download at the CDC National Center for Health Statistics at https://www.cdc.gov/nchs/nhanes/index.htm.\n",
    "\n",
    "\n",
    "- Each NHANES 2013-2014 raw dataset is stored in SAS transport file format (XPT) and corresponds uniquely to a single questionnaire provided to respondents. Surveyed individuals are assigned a unique identifier number (SEQN) which serves as record index. The table below summarizes the datasets uses, the raw attributes and their description.\n",
    "\n",
    "\n",
    "- Demographic and questionnaire column labels were renamed. The new labels were selected according to their description in the dataset documentation. Biochemistry feature column labels were not changed. See table below for the description of each label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig1.png\"\n",
    "     alt=\"FeaturesTable\"\n",
    "     style=\"float: left; margin-right: 10px; width: 850px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health Status Questionnaire Data\n",
    "\n",
    "\n",
    "The questionnaires gather health status and lifestyle behavior that we consider as potential predictors for cardiovascular disease and diabetes.\n",
    "\n",
    "The table belows shows the colums and features corresponding to health status records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig2.png\"\n",
    "     alt=\"FeaturesTable\"\n",
    "     style=\"float: left; margin-right: 10px; width: 950px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blood Standard Biochemistry Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard biochemistry data contains a series of measurements used in the diagnosis and treatment of certain liver, heart, and kidney diseases; acid-base imbalance in the respiratory and metabolic systems; other diseases involving lipid metabolism; various endocrine disorders; as well as other metabolic or nutritional disorders. We also consider these potential predictors for cardiovascular disease and diabetes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig3.png\"\n",
    "     alt=\"FeaturesTable\"\n",
    "     style=\"float: left; margin-right: 10px; width: 950px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plots in Fig. 4 show the distribution of individuals presenting high values of cholesterol, risk for diabetes, hypertension and prediabetes (see bars in red).  \n",
    "\n",
    "In all cases, the classes are imbalanced, as indicated by the corresponding class ratios.  Class imbalance is also evidenced when we divide the data by groups, e.g.  by age or body mass index as shown in Fig. 5.  \n",
    "\n",
    "These exploratory results suggest using classification evaluation metrics adequate for class imbalanced problems such as ROC-AUC score, recall, specificity, f1-score and illustrative tools such as confusion matrices and ROC curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig4.png\"\n",
    "     alt=\"FeaturesTable\"\n",
    "     style=\"float: left; margin-right: 10px; width: 950px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig5.png\"\n",
    "     alt=\"FeaturesTable\"\n",
    "     style=\"float: left; margin-right: 10px; width: 950px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use AUC ROC as the primary metric to determine classification performance, while considering complementary metrics typically used in clinical settings. In our case, the target feature is a disease. \n",
    "\n",
    "From the cohort of surveyed individuals is reasonable to assume that the prevalent class are the healthy subjects (negative cases), while the diseased belong to the rare class (positive cases). The rare class is the class of more interest, and is typically designated 1 (presenting disease), in contrast to the more prevalent 0s (not presenting disease).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Under ROC Curve. \n",
    "\n",
    "- This is our main metric. ROC curve in itself doesnâ€™t constitute a single measure for classifier performance. AUC is simply the total area under the ROC curve. The larger the value of AUC, the more effective the classifier. \n",
    "\n",
    "\n",
    "- When using normalized units, AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ROC Curve\n",
    "\n",
    "- A plot of recall vs. specificity.  For problems with imbalanced classes there is always trade-off between recall and specificity as we change the cutoff to determine how to classify a record. This indicator is directly related to the ROC-AUC score.\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "As shown in Fig. 6, the confusion matrix is a table showing the number of correct and incorrect predictions categorized by type of response. This indicator is directly related to the metrics precision, recall, specificity and f1-score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig6.png\"\n",
    "     alt=\"ConfusionMatrix\"\n",
    "     style=\"float: center; margin-center: 10px; width: 350px;\" />\n",
    "     \n",
    "**Figure 6 ** Confusion matrix schematics showing some of the related metrics considered in this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "\n",
    "- This metric measures the strength of the model to predict a positive outcome (disease) â€” the proportion of the 1s that it correctly identifies.\n",
    "\n",
    "\n",
    "$$ \\it Recall = \\frac{\\sum \\text{True Positives}}{\\sum \\text{True Positives} + \\sum \\text{False Negatives} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specificity\n",
    "\n",
    "- This metric measures the strength of the model to predict a negative outcome (no presenting disease) â€” the proportion of the 0s that it correctly identifies.\n",
    "\n",
    "$$ \\it Recall = \\frac{\\sum \\text{True Negatives}}{\\sum \\text{True Negatives} + \\sum \\text{False Negatives} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "- This metric measures the accuracy of a predicted positive outcome (presenting disease). \n",
    "\n",
    "$$ \\it Recall = \\frac{\\sum \\text{True Positives}}{\\sum \\text{True Positives} + \\sum \\text{False Positives} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "- This metric measures the accuracy of a predicted positive outcome (presenting disease). \n",
    "\n",
    "$$ \\it Recall = \\frac{\\sum \\text{True Positives} +\\sum \\text{True Negatives}}{\\sum \\text{True Positives} + \\sum \\text{False Positive} +\\sum \\text{True Negatives} + \\sum \\text{False Negatives} } $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $F_1$ score\n",
    "\n",
    "- This metric is the harmonic average of the precision and recall, reaching its best value at 1 (perfect precision and recall) and worst at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ 2 \\times \\frac{\\text{Precision}\\times\\text{Recall}}{\\text{Precision}+\\text{Recall}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve our classification problem, we make use of **XGBoost (Extreme Gradient Boosting)**, which is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework [7].\n",
    "\n",
    "More generally, **Gradient Boosting** is an *ensemble machine learning technique* for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models (typically decision trees for XGBoost). Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models [8].\n",
    "\n",
    "\n",
    "\n",
    "Main justifications for the problem at hand:\n",
    "\n",
    "1.\tFaster than normal Gradient Boosting as it implements parallel processing. \n",
    "2.\tUseful for unbalanced classes. Controls the balance of the positive and negative weights, via scale_pos_weight hyper-parameter tuning.\n",
    "3.\tAbility to use AUC for optimization and evaluation.\n",
    "4.\tProvides a built-in feature importance ranking algorithm.\n",
    "5.\tScalable to big-data applications. Permits scaling up the problem discussed in this project for the case of integrating larger healthcare datasets from multiple sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "To identify the important features i.e. risk factors that serve as main predictors of disease, we use a feature importance measure built-in within the XGBoost framework.  This measure is calculated for a single decision tree using the F-score, which is the amount that each attribute split point improves the performance measure (AUC), weighted by the number of observations the node is responsible for. \n",
    "\n",
    "Generally, the feature importance score indicates how useful or valuable each feature was in the construction of the boosted decision trees within the model. The more an attribute is used to make key decisions with decision trees, the higher its relative importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization techniques\n",
    "\n",
    "#### Incremental Grid Search with Cross Validation\n",
    "\n",
    "We implemented optimization of Xgboost hyper-parameters using cross-validated grid search, methodically building and evaluating a model for each combination of parameters specified in a grid. To control for overfitting, a stratified k-fold cross validation was applied in each iteration of grid-search. \n",
    "\n",
    "#### Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "We tested this technique to create synthetic data with aim of mitigating class imbalance. SMOTE selects two or more similar data instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances. SMOTE can achieve better classifier performance (in ROC space).\n",
    "\n",
    "#### Auxiliary testing algorithms\n",
    "\n",
    "Preprocessing and scaling. We developed and implemented a series of algorithms to perform all the necessary data cleaning, feature scaling and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Datasets Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing step was the most time-consuming in the present project. Given the imbalanced nature of the classification problem at hand, it is critical that the curated dataset contains enough and consistent record values expressed in a useful scale and format. Also, we want to consider only the features relevant to our problem. The following preprocessing pipeline was implemented:\n",
    "\n",
    "\n",
    "### General procedure\n",
    "\n",
    "- Read each SAS file and convert it to a pandas dataframe.\n",
    "\n",
    "\n",
    "- Restrict dataframe to relevant features.\n",
    "\n",
    "\n",
    "- Identify codes representing missing value (different SAS datasets use different codes for missing values).\n",
    "\n",
    "\n",
    "- Data imputation. Remove missing values or replace missing values with zeroes, or mean values when appropriate. \n",
    "\n",
    "\n",
    "- Mitigate dataset biasing by imputation procedures. Data imputation on missing values was done after ensuring they were less than 10% of data; in any other case, data was done with mean values when appropriate. In some instances, we removed entire rows when several missing values occurred across columns.\n",
    "\n",
    "\n",
    "- One hot encoding of categorical features when appropriate.\n",
    "\n",
    "\n",
    "- Recoding and reordering ordinal features when not ordered in original dataset.\n",
    "\n",
    "\n",
    "- Perform normalization and feature scaling according to the nature of the data presented in each data set. This step necessitated the creation of custom processing algorithms to achieve scaling appropriate for each dataset. \n",
    "\n",
    "\n",
    "- Ensure dataframes consistency in terms equal number of records. \n",
    "\n",
    "\n",
    "- Integrate all dataframes into a single consistent dataframe free of missing values, with all features appropriately re-scaled. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows our implementation flow chart indicating the processes and algorithms involved in determining the most important features that predict the risk for cardiovascular disease and diabetes. \n",
    "\n",
    "<img src=\"workflow.png\"\n",
    "     alt=\"ModelingWorkflow\"\n",
    "     style=\"float: center; margin-center: 10px; width: 650px;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preliminary Analysis\n",
    "\n",
    "### Correlation Matrices and Heatmaps\n",
    "\n",
    "To gain preliminary insight about important predictors for CVD and Diabetes, we constructed a pair of correlation matrices. The correlations were computed using point biserial correlation coefficient. The point biserial correlation coefficient *pointbiserialr(Y,X)* is a correlation coefficient used when one variable Y is dichotomous (binary) and the other X is either continuous or dichotomous [9].\n",
    "\n",
    "The figure below shows a correlation heat map indicating the correlation between health status features as defined in questionnaire data records. \n",
    "\n",
    "\n",
    "<img src=\"heatmap1.png\"\n",
    "     alt=\"Heatmap1\"\n",
    "     style=\"float: center; margin-center: 10px; width: 750px;\" />\n",
    "\n",
    "\n",
    "The following observations highlight the validity of preliminary results, and indicate robust data quality:\n",
    "\n",
    "-\tIn particular, we notice a cluster of high positive correlation (center of heat map) between Hypertension Onset, High Cholesterol Onset, High Cholesterol, Hypertension, Chest discomfort, Chest Pain and Shortness of Breath, which collectively conform the signatures of Cardiovascular Disease (CVD). \n",
    "\n",
    "-\tUsing this insight, we engineered the CVD target label 'CARDIO_DISORDER' defined as the text the weighted sum of 'BREATH_SHORTNESS', 'CHEST_PAIN_30MIN', 'CHEST_DISCOMFORT', 'HYPERTENSION', and 'HYPERTENSION_ONSET'. \n",
    "\n",
    "-\t'AGE', 'BMI', DIAGNOSED_DIABETES' shows a significant positive correlation with these components. Indicating relationship between CVD and increased age and increased body mass index (obesity). Coupled to the above, 'FAST_FOOD', and 'NOT_HOME_FOOD' (a person's habit to eat pre-processed foods) correlates negatively with CVD indicators.\n",
    "\n",
    "The figure below shows a correlation heat map showing the correlation between standard biochemistry profile features and features related to CVD and Diabetes predictors. \n",
    "\n",
    "<img src=\"heatmap2.png\"\n",
    "     alt=\"Heatmap1\"\n",
    "     style=\"float: center; margin-center: 10px; width: 750px;\" />\n",
    "\n",
    "\n",
    "The following observations highlight the validity of the preliminary results, and indicate excellent robust data quality:\n",
    "\n",
    "-\tAgain, we notice the same cluster of high positive correlation among CVD and Diabetes predictors shown in Fig. 12, but now located on the left upper corner of the heat map.\n",
    "\n",
    "-\t'LBXSGL' (Glucose level) positively correlates strongly with 'DIAGNOSED_DIABETES'\n",
    "\n",
    "-\t'LBXSAL' (Albumin level) negatively correlates with the CVD and Diabetes predictors, BMI, High Cholesterol Onset, High Cholesterol, Hypertension, Chest discomfort, Chest Pain and Shortness of Breath. Low albumin blood levels, even low normal, correlate with cellular inflammation and increased risk of death from cardiovascular disease (CVD,) such as coronary heart disease and strokes [https://labtestsonline.org/tests/albumin].\n",
    "\n",
    "-\t'LBXSCLSI' negatively correlates with the CVD and Diabetes predictors. A decreased level of blood chloride (called hypochloremia) occurs with any disorder that causes low blood sodium. Hypochloremia also occurs with congestive heart failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will benchmark qualitatively and semi-quantitatively our results with recent studies by Weng [5] and Alghamdi [6]. These authors conducted analyses on big clinical datasets and ranked the most important risk factors for developing diabetes and cardiovascular disease using a large cohort of individuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Appendix 1: Data pre-processing and feature scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionnaire Data\n",
    "\n",
    "**Pre-processing demographics data (DEMO_H.XPT)\n",
    "Resultant features: ['AGE','INCOME_LEVEL', 'ETHNICITY_Black', 'ETHNICITY_Hispanic', 'ETHNICITY_White']**\n",
    "\n",
    "-\tWe restricted the AGE feature column to include only records where respondent's age is between 18 and 65.\n",
    "\n",
    "-\tData imputation was done on the INCOME LEVEL feature by replacing missing values with mean income. \n",
    "-\tRe-encoding was done to reorder income level in increasing order.\n",
    "-\tRe-encoding was done to replace ETHNICITY integer encoding with string values (ETHNICITY White, ETHNICITY Asian, etc.).\n",
    "-\tOne-hot encoding of final categorical labels GENDER, ETHNICITY\n",
    "\n",
    "**Pre-processing alcohol consumption level data (ALQ_H.XPT) \n",
    "Resultant feature: ['ALCOHOL_NUM']**\n",
    "\n",
    "-\tData imputation was done to replace missing values (unknown consumption) with 0 (person doesn't drink).\n",
    "-\tData imputation was done to fill missing values resulting from re-indexing with mean values.\n",
    "\n",
    "**Pre-processing smoking behavior data (SMQ_H.XPT)\n",
    "Resultant feature: ['SMOKING']**\n",
    "\n",
    "-\tData imputation was done to replace missing values (unknown consumption) with 0 (person doesn't smoke).\n",
    "-\tData imputation was done to fill any smoking consumption level with 1 (person smokes).\n",
    "\n",
    "**Pre-processing body weight and height data (WHQ_H.XP)\n",
    "Resultant feature: ['BMI']**\n",
    "\n",
    "-\tThe body-mass index (BMI) feature was not present in original datasets. Only Height h and Weight w were available. Body mass index was calculated using a well-known formula to estimate it. We use it the following function\n",
    "\t\t\t\t\tdef bmi(h, w):\n",
    "\t\t\t\t\t\treturn np.round((w/h**2) * 703.0, 2)\n",
    "\n",
    "**Pre-processing nutrition and eating behavior data (DBQ_H.XPT)\n",
    "Resultant features: ['NOTHOME_FOOD', 'FAST_FOOD']**\n",
    "\n",
    "-\tData imputation was done to replace missing values in original survey answers with mean value of corresponding feature.\n",
    "-\tData transformation was done to convert food consumption rates to percentage values. If x is number of days where person consumed fast food over a n-day period, then the percent is obtained with\n",
    "\n",
    "\t\t\t\t\tdef percent_transform(x,n):\n",
    "    \t\t\t\t\t\treturn np.round(float(x/n *100.0),2)\n",
    "\n",
    "-\tData imputation was done to fill missing values resulting from re-indexing with mean values.\n",
    "\n",
    "**Pre-processing cholesterol and blood pressure status data (BPQ_H.XPT)\n",
    "Resultant features: ['HYPERTENSION_ONSET', 'HYPERTENSION', 'HIGHCHOL_ONSET', 'HIGHCHOL']**\n",
    "\n",
    "-\tThere were three features in original dataset indicating that a person had hypertension. We combined these features into the feature 'HYPERTENSION' using an OR operand across the feature columns.\n",
    "\n",
    "-\tSince 'HYPERTENSION' is an important predictor for CVD and Diabetes, we replaced missing values with zero to avoid biasing.\n",
    "\n",
    "**Pre-processing health insurance data (HIQ_H.XPT)\n",
    "Resultant features: ['INSURANCE']**\n",
    "\n",
    "-\tData imputation was done to replace missing values (unknown insurance status) with 0 (person doesn't have insurance).\n",
    "\n",
    "**Pre-processing cardiovascular health status data (CDQ_H.XPT)\n",
    "Resultant features: ['CHEST_DISCOMFORT', 'CHEST_PAIN_30MIN', 'BREATH_SHORTNESS']**\n",
    "\n",
    "-\tThe features in this group are predictors for CVD and Diabetes, we replaced missing values with zero to avoid biasing.\n",
    "\n",
    "**Pre-processing diabetes status data (DIQ_H.XPT)\n",
    "Resultant features: ['FAMILIAL_DIABETES', 'DIAGNOSED_DIABETES', 'DIAGNOSED_PREDIABETES', 'RISK_DIABETES']**\n",
    "\n",
    "-\tThere were three different possible values of DIAGNOSED_DIABETES that indicated diabetes, and other three which indicated negative for diabetes. We set all this values to 1 (positive for diabetes) and 0 (negative for diabetes, respectively)\n",
    "-\tAll missing values were replaced by zeroes.\n",
    "\n",
    "**Pre-processing miscellaneous continuous variables \n",
    "Resultant features: ['AGE', 'INCOME_LEVEL', 'ALCOHOL_NUM', 'BMI', 'NOTHOME_FOOD', 'FAST_FOOD']**\n",
    "\n",
    "-\tWe applied a log transform (to alleviate skewness) and scaling via sklearn's RobustScaler(), which scales features to same scale using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Biochemistry Profile Data\n",
    "\n",
    "**Pre-processing blood biochemistry continuous variables (BIOPRO_H.XPT)\n",
    "Resultant features: all features in dataset**\n",
    "\n",
    "-\tWe applied a log transform (to alleviate skewness) and robust scaling via Sklearn's RobustScaler(), which scales features to same scale using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes Merger\n",
    "\n",
    "All processed questionnaire and laboratory dataframes were integrated into a single dataframe nhanes_df. We performed a final check to ensure no missing values nor un-scaled features where present in the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
